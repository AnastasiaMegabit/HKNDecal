{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - SF vs NYC Housing \n",
    "#### Going Down the EECS Stack DeCal Spring 2017\n",
    "#### Data courtesy of [r2d3](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)\n",
    "\n",
    "<img src=\"https://grapecollective.com/media/article/image/cache/720x337-center/c/o/comparison.jpg\">\n",
    "\n",
    "In this notebook, we'll explore some ideas behind machine learning using housing data from San Francisco and New York. </br>\n",
    "Each data point corresponds to a house with the following fields:\n",
    "- year_build (int, e.g. 1990)\n",
    "- price_per_sqft (int, dollars)\n",
    "- bath (float)\n",
    "- beds (float)\n",
    "- elevation (int, ft)\n",
    "- price (int, dollars)\n",
    "- in_sf (int, 1 if in sf, 0 otherwise)\n",
    "\n",
    "In the first part of this notebook we'll try to classify houses and predict whether a house is from NYC or SF. In the second part we'll try to regress and predict the prices of the houses.\n",
    "\n",
    "<br />\n",
    "Mathy Notation for later parts:\n",
    "- $n$ denotes the number of data points (houses) \n",
    "- $d$ denotes the number of features \n",
    "- $X$ is an $n \\times d$ matrix, where each row corresponds to a house. $X_i$ means the $i$th row, or the $i$th feature vector.\n",
    "- $y$ is a length $n$ vector, where each index corresponds to a label. $y_i$ means the label for the $i$th house. For part 1 the labels are either $1$ or $0$ for SF and NYC respectively. For part 2 the labels are real numbers denoting housing price.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "from util import extract_cols, visualize_linear_regression, visualize_perceptron, load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load housing data\n",
    "all_data, features_c, labels_c, features_r, labels_r = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Classification: NYC vs SF\n",
    "In this section, our goal is to learn a model that predicts whether a given house is from SF or from NYC. Our label is \"is_sf\", which is 1 if the house belongs to SF, 0 if NYC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1 Manual Classification\n",
    "1. Use the cell below to explore statistics about the housing data. You can look for things such as \"average elevation of houses in SF\" vs \"average elevation of houses in NY\".\n",
    "2. Use your knowledge about the data to fill in the function \"is_in_sf,\" which takes in a feature dictionary and returns 1 if you think this house belongs in SF, 0 if it belongs to NY\n",
    "3. Test how good your result is. Try to improve your score as much as possible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean elevation of houses in NYC is 13.5044642857\n",
      "The mean elevation of houses in SF is 61.8619402985\n"
     ]
    }
   ],
   "source": [
    "# 1) explore the data\n",
    "def query(in_sf, feature_name, statistic):\n",
    "    '''\n",
    "    in_sf is either 1 or 0\n",
    "    statistic is either 'mean', 'std', 'min', 'max'\n",
    "    feel free to modify this function to explore other properties about the housing data.\n",
    "    '''\n",
    "    if in_sf not in (0, 1):\n",
    "        raise ValueError(\"Location can only be 0 for nyc or 1 for sf!\")\n",
    "    if feature_name not in features_c[0]:\n",
    "        raise ValueError(\"Invalid feature_name!\")\n",
    "    if statistic not in ('mean', 'std', 'max', 'min'):\n",
    "        raise ValueError(\"Statistic can only be 'mean', 'std', 'min', 'max'\")\n",
    "    \n",
    "    col = []\n",
    "    for data in all_data:\n",
    "        if data['in_sf'] == in_sf:\n",
    "            col.append(data[feature_name])\n",
    "    \n",
    "    statistics_map = {\n",
    "        'min': np.min,\n",
    "        'max': np.max,\n",
    "        'mean': np.mean,\n",
    "        'std': np.std\n",
    "    }\n",
    "    \n",
    "    return statistics_map[statistic](col)\n",
    "    \n",
    "print \"The mean elevation of houses in NYC is\", query(0, 'elevation', 'mean')\n",
    "print \"The mean elevation of houses in SF is\", query(1, 'elevation', 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2) Implement is_in_sf\n",
    "def is_in_sf(feature):\n",
    "    '''\n",
    "    feature is a dictionary with the following keys: \n",
    "    - 'year_built'\n",
    "    - 'price_per_sqft'\n",
    "    - 'bath'\n",
    "    - 'beds'\n",
    "    - 'elevation'\n",
    "    - 'price'\n",
    "    \n",
    "    return 1 if house is predicted to be in SF, 0 if NYC\n",
    "    '''\n",
    "    ### Your Code Below ###\n",
    "    # SAMPLE CODE #\n",
    "    if feature['elevation'] > 30:\n",
    "        return 1\n",
    "    else:\n",
    "        0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 37.20% correct!\n",
      "Out of incorrect predictions, 27.51% were SF houses predicted to be in NYC\n"
     ]
    }
   ],
   "source": [
    "# 3) Test your performance!\n",
    "num_correct = 0\n",
    "sf_but_pred_ny = 0\n",
    "total_num = len(features_c)\n",
    "for i, feature in enumerate(features_c):\n",
    "    prediction_in_sf = is_in_sf(feature)\n",
    "    if prediction_in_sf == labels_c[i]:\n",
    "        num_correct += 1\n",
    "    else:\n",
    "        if labels_c[i] and not prediction_in_sf:\n",
    "            sf_but_pred_ny += 1\n",
    "print \"Got {:.2f}% correct!\".format(num_correct/1./total_num*100)\n",
    "print \"Out of incorrect predictions, {:.2f}% were SF houses predicted to be in NYC\".format(\n",
    "                                                                                sf_but_pred_ny/1./(total_num - num_correct)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2 The Perceptron Algorithm\n",
    "\n",
    "In this section we will implement the perceptron algorithm, which will learn a linear decision boundary function $f(x)$ of the form:\n",
    "$$\n",
    "f(X_i) = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1 & \\quad w^\\top X_i + b > 0 \\\\\n",
    "            0 & \\quad else\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$$\n",
    "You can think of this function as drawing a line in the feature space. If a data point is above this line, we'll say it's from SF. If a data point is below this line, we'll say it's from NYC. In the 1D case where there is only 1 feature used, $w$ would be the slope of the line, and $b$ the y-intercept.\n",
    "\n",
    "<br />\n",
    "\n",
    "Note that $f$ is *parameterized* by $w$ and $b$. So our goal is to find the $w$ and $b$ that best minimizes a *loss* function:\n",
    "$$\n",
    "L(X, y) = \\frac{1}{N} \\sum_{i=1}^{N} |f_{w, b}(X_i) - y_i|\n",
    "$$\n",
    "\n",
    "<br />\n",
    "The perceptron algorithm has 2 *hyperparameters*: the learning rate and the number of epochs to be trained on. In addition, we can also select a smaller set of features instead of using all of them to learn on (sometimes this performs better). Complete the steps below:\n",
    "1. Understand the perceptron algorithm implementation\n",
    "2. Experiment w/ learning rate and epochs. How do these affect the train and test performance? Why?\n",
    "3. Experiment w/ feature selection. Which features seem to work the best? Why?\n",
    "4. Tweak the above settings to get the best test performance. \n",
    "5. If you used 2 or 3 features, you can run the visualization code to visualize the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def f(w, b, x):\n",
    "    if w.dot(x) + b > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def classification_accuracy(X, y, w, b):\n",
    "    y_pred = [f(w, b, x) for x in X]\n",
    "    return (1 - mean_absolute_error(y_pred, y))*100\n",
    "\n",
    "def regression_mae(X, y, w, b):\n",
    "    y_pred = X.dot(w) + b\n",
    "    return mean_absolute_error(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1) Read this implementation. Try matching it with the pseudocode\n",
    "def perceptron_learn_w_b(X_tr, y_tr, X_t, y_t, epochs, learning_rate):\n",
    "    '''\n",
    "    Run the perceptron algorithm for epochs iterations\n",
    "    Return w, b\n",
    "    '''\n",
    "    # dimensions\n",
    "    N = X_tr.shape[0] # number of data points we have\n",
    "    d = X_tr.shape[1] # dimension of a feature vector\n",
    "    \n",
    "    # initialize weights\n",
    "    w = np.zeros(d) # a vector of 0's of size d\n",
    "    b = 0 # bias starts at 0\n",
    "    \n",
    "    # perceptron learning algorithm\n",
    "    for t in range(epochs):\n",
    "        for i in range(N):            \n",
    "            x = X_tr[i]\n",
    "            pred_y = f(w, b, x)\n",
    "            error = y_tr[i] - pred_y\n",
    "            \n",
    "            # TODO: HIDE THIS FOR IMPLEMENTATION??\n",
    "            b = b + learning_rate * error\n",
    "            w = w + learning_rate * error * x\n",
    "        \n",
    "        # reporting accuracy\n",
    "        train_accuracy = classification_accuracy(X_tr, y_tr, w, b)\n",
    "        test_accuracy = classification_accuracy(X_t, y_t, w, b)\n",
    "        clear_output(wait=True)\n",
    "        print 'epoch={}/{}'.format(t+1, epochs), 'train={:.2f}%'.format(train_accuracy), 'test={:.2f}%'.format(test_accuracy)\n",
    "                \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2) Choose hyperparameters\n",
    "epochs = 100\n",
    "learning_rate = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3) Choose which features to use\n",
    "# available features are 'year_built', 'price_per_sqft', 'bath', 'beds', 'elevation', 'price'\n",
    "features_to_use_c = ['elevation', 'price_per_sqft', 'bath', 'beds', 'elevation', 'price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=100/100 train=46.82% test=40.40%\n",
      "w is  [  834777.   2758516.     18145.5    25886.    834777.    -91700. ] b is  9501\n"
     ]
    }
   ],
   "source": [
    "# 4) Run this cell to train perceptron!\n",
    "index_train_split = int(0.8 * len(features_c))\n",
    "features_c_small = extract_cols(features_c, features_to_use_c)\n",
    "features_c_train_array, features_c_test_array = features_c_small[:index_train_split], features_c_small[index_train_split:]\n",
    "labels_c_train_array, labels_c_test_array = np.array(labels_c[:index_train_split]), np.array(labels_c[index_train_split:])\n",
    "\n",
    "w_c, b_c = perceptron_learn_w_b(features_c_train_array, labels_c_train_array, features_c_test_array, labels_c_test_array, \n",
    "                            epochs, learning_rate)\n",
    "print 'w is ', w_c, 'b is ', b_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualize 2d can only take 2 or 3 features at once!\n"
     ]
    }
   ],
   "source": [
    "# 5) visualize data and learned decision boundary\n",
    "visualize_perceptron(features_to_use_c, features_c_train_array, labels_c_train_array, w_c, b_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Regression on Housing Price\n",
    "\n",
    "In this section, our goal is to learn a model that predicts house prices. Our label is now \"price\" instead of \"in_sf.\" \"in_sf\" is now included as a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1 Linear Regression for Expected House Price\n",
    "\n",
    "In linear regression, we use a linear function to map from input features to output labels. Similar to the perceptron algorithm above, this model has the form:\n",
    "$$\n",
    "y_i = w^\\top X_i + b\n",
    "$$\n",
    "The new $f$ for regression which outputs the predicted $y$ values will be:\n",
    "$$\n",
    "f(X) = Xw + B\n",
    "$$\n",
    "Where $B$ is a length $n$ vector, and every single value is $b$. Our loss function will be the same as above:\n",
    "$$\n",
    "L(X, y) = \\frac{1}{N} \\sum_{i=1}^{N} |f_{w, b}(X_i) - y_i|\n",
    "$$\n",
    "\n",
    "Complete the steps below:\n",
    "1. Implement Linear Regression using $b$ as the mean of $y$ and $w$ the pseudoinverse of $X$\n",
    "2. Experiment with which features to use to get the best performance\n",
    "3. Run Linear Regression. Observe results. MAE stands for Mean Absolute Error. \n",
    "4. Visualize the linear regression line for when using only 1 feature or 2 features.\n",
    "5. Are the results good or bad? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1) Implement Linear Regression\n",
    "def linear_regression(X_tr, y_tr, X_t, y_t):\n",
    "    '''\n",
    "    return weight vector w and bias b\n",
    "    '''\n",
    "    # TODO: ERASE FOR IMPLEMENTATION?\n",
    "    b = y_tr.mean()\n",
    "    \n",
    "    w = np.linalg.inv(X_tr.T.dot(X_tr)).dot(X_tr.T).dot(y_tr - b)\n",
    "    \n",
    "    train_mae = regression_mae(X_tr, y_tr, w, b)\n",
    "    test_mae = regression_mae(X_t, y_t, w, b)\n",
    "    \n",
    "    print \"Train MAE {}\".format(train_mae), \"Test MAE {}\".format(test_mae)   \n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2) Choose which features to use\n",
    "features_to_use_r = ['price_per_sqft', 'year_built', 'bath', 'beds', 'elevation', 'in_sf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MAE 740438.642185 Test MAE 850913.904033\n",
      "w is  [  2.76338481e+03  -2.87204623e+03   7.80270315e+05   2.40583926e+05\n",
      "   5.51170228e+02   4.22113454e+05] b is  2039970.8855\n"
     ]
    }
   ],
   "source": [
    "# 3) Run this cell to run linear regression\n",
    "index_train_split = int(0.8 * len(features_r))\n",
    "features_r_small = extract_cols(features_r, features_to_use_r)\n",
    "features_r_train_array, features_r_test_array = features_r_small[:index_train_split], features_r_small[index_train_split:]\n",
    "labels_r_train_array, labels_r_test_array = np.array(labels_r[:index_train_split]), np.array(labels_r[index_train_split:])\n",
    "\n",
    "w_r, b_r = linear_regression(features_r_train_array, labels_r_train_array, features_r_test_array, labels_r_test_array)\n",
    "print 'w is ', w_r, 'b is ', b_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can only visualize 1 or 2 features at a time.\n"
     ]
    }
   ],
   "source": [
    "# 5) Visualize linear predictor for 1 or 2 features\n",
    "visualize_linear_regression(features_to_use_r, features_r_train_array, labels_r_train_array, w_r, b_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Part 2.2 Regression with Decision Tree\n",
    "\n",
    "The main drawback of linear regression is that it is a model with low expressiveness (or representational power) - it can't fit to complex patterns in data. Another popular method in supervised learning is called Decision Tree. Below we demonstrate fitting this data using decision trees and show the improvements in prediction.\n",
    "\n",
    "<br />\n",
    "\n",
    "First, because mean absolute error can be difficult to interpret, we introduce another metric called the [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination), or r2 score. R2 scores roughly compute how good a set of predictions are given ground truth data. A higher r2 score means better predictions, and 100% accuracy correspond to an r2 score of 1. \n",
    "\n",
    "<br />\n",
    "Complete the following steps:\n",
    "1. Compute r2 scores for linear regressor\n",
    "2. Run Decision Tree Regressor\n",
    "3. Compute r2 scores for decision tree\n",
    "4. Export and visualize decision tree using dot. Then run the command below in terminal to generate a png file. Compare results with your neighbors. Which features are being split on? Which feature is the most important one?\n",
    "\n",
    "```bash\n",
    "dot -Tpng decision_tree_regressor.dot -o tree.png\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regressor | Train r2 0.81. Test r2 0.19\n"
     ]
    }
   ],
   "source": [
    "# 1) Compute linear regressor's r2 score using the linear regressor above\n",
    "pred_tr_linear = features_r_train_array.dot(w_r) + b_r\n",
    "pred_t_linear = features_r_test_array.dot(w_r) + b_r\n",
    "pred_tr_linear_r2 = r2_score(pred_tr_linear, labels_r_train_array)\n",
    "pred_t_linear_r2 = r2_score(pred_t_linear, labels_r_test_array)\n",
    "print \"Linear Regressor | Train r2 {:.2f}. Test r2 {:.2f}\".format(pred_tr_linear_r2, pred_t_linear_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "           splitter='best')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Train a Decision Tree Regressor\n",
    "decision_tree = DecisionTreeRegressor()\n",
    "decision_tree.fit(features_r_train_array, labels_r_train_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Regressor | Train r2 1.00. Test r2 0.65\n"
     ]
    }
   ],
   "source": [
    "# 3) Compute decision tree regressor's r2 score\n",
    "pred_tr_decisiontree = decision_tree.predict(features_r_train_array)\n",
    "pred_t_decisiontree = decision_tree.predict(features_r_test_array)\n",
    "pred_tr_decisiontree_r2 = r2_score(pred_tr_decisiontree, labels_r_train_array)\n",
    "pred_t_decisiontree_r2 = r2_score(pred_t_decisiontree, labels_r_test_array)\n",
    "print \"Decision Tree Regressor | Train r2 {:.2f}. Test r2 {:.2f}\".format(pred_tr_decisiontree_r2, pred_t_decisiontree_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4) export decision tree\n",
    "export_graphviz(decision_tree, out_file='decision_tree_regressor.dot', feature_names=features_to_use_r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
